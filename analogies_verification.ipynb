{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verifying \"Analogies Explained\" via Synthetic Data\n",
    "\n",
    "This notebook verifies the paper by constructing synthetic co-occurrence data where paraphrases hold by design.\n",
    "\n",
    "**Core equation:** For $w^* = W$ (e.g., king = {man, royalty}):\n",
    "\n",
    "$$\\text{PMI}(c, w^*) = \\sum_{w \\in W} \\text{PMI}(c, w) + \\rho(c) + \\sigma(c) - \\tau$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Generation.** \n",
    "\n",
    "This simulation creates a synthetic corpus where paraphrase structure is **explicitly embedded by design**:\n",
    "\n",
    "- **Target words**: `man`, `woman`, `king`, `queen`, `<royalty>`\n",
    "- **Context pools**: Each target has its own context distribution (e.g., `king` draws from `MAN + ROYALTY`)\n",
    "- **Paraphrase pairs**: `P(c|king) ≈ P(c|man, royalty)` and `P(c|queen) ≈ P(c|woman, royalty)` by construction\n",
    "\n",
    "All what we count here will be used later to calculate needed probabilities, e.g. `P(W|c)`, `P(W)`, `P(c|W)` for `W=(man,royalty)` etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> I chose x = 1/6 to make τ = 0 (marginal independence: p(man, royalty) = p(man)·p(royalty)). Let p = 5x be the probability of sampling a single target. Then:\n",
    "> - p(man) = p/5 + (1-p)/2\n",
    "> - p(royalty) = p/5 + (1-p)\n",
    "> - p(man, royalty) = (1-p)/2\n",
    "> \n",
    "> Solving p(man)·p(royalty) = p(man, royalty) gives x = 1/6.\n",
    ">\n",
    "> Note: This choice is convenient but not essential. Due to symmetry in our construction (man/woman and king/queen are parallel), we have τ_king ≈ τ_queen, so their difference cancels in the analogy regardless of the individual τ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n_windows=100_000, x=1/6, seed=42, scale=10):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "    NEUTRAL = [f\"neutral_{i}\" for i in range(10 * scale)]\n",
    "    MAN = [f\"man_{i}\" for i in range(4 * scale)]\n",
    "    WOMAN = [f\"woman_{i}\" for i in range(4 * scale)]\n",
    "    ROYALTY = [f\"royalty_{i}\" for i in range(2 * scale)]\n",
    "    \n",
    "    CONTEXTS = {\n",
    "        'man': MAN + NEUTRAL,\n",
    "        'woman': WOMAN + NEUTRAL,\n",
    "        'king': MAN + ROYALTY,\n",
    "        'queen': WOMAN + ROYALTY,\n",
    "        '<royalty>': ROYALTY,\n",
    "        ('<royalty>', 'man'): MAN + ROYALTY,\n",
    "        ('<royalty>', 'woman'): WOMAN + ROYALTY,\n",
    "    }\n",
    "    TARGETS = ['man', 'woman', 'king', 'queen', '<royalty>']\n",
    "    \n",
    "    counts = defaultdict(int)\n",
    "    paired_counts = defaultdict(int)\n",
    "    target_conditioned_counts = defaultdict(lambda: defaultdict(int))\n",
    "    \n",
    "    for _ in range(n_windows):        \n",
    "        if random.random() < 5 * x:\n",
    "            targets = [random.choice(TARGETS)]\n",
    "            ctx_key = targets[0]\n",
    "        else:\n",
    "            targets = random.choice([['man', '<royalty>'], ['woman', '<royalty>']])\n",
    "            ctx_key = tuple(sorted(targets))\n",
    "            paired_counts[ctx_key] += 1\n",
    "        \n",
    "        context = list(set(random.sample(CONTEXTS[ctx_key], k=4)))\n",
    "        \n",
    "        for w0 in targets:\n",
    "            counts[w0] += 1\n",
    "            for w1 in context:\n",
    "                paired_counts[tuple(sorted((w0, w1)))] += 1\n",
    "        \n",
    "        for i, w1 in enumerate(context):\n",
    "            counts[w1] += 1\n",
    "            target_conditioned_counts[ctx_key][w1] += 1\n",
    "            for j in range(i+1, len(context)):\n",
    "                w2 = context[j]\n",
    "                paired_counts[tuple(sorted((w1, w2)))] += 1\n",
    "                \n",
    "    return counts, paired_counts, target_conditioned_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PMI and Embeddings**\n",
    "> Remember, $W$ is paraphrase context and $w$ is usually a target word, e.g., `W=(man, royalty)` and `w=king`\n",
    "\n",
    "**Comment on smoothing.** Vanilla PMI calculation has to deal with log(0) when we don't meet context word $c$. Laplace $\\alpha$-smoothing $p(c) = \\frac{N_c + \\alpha}{N + \\alpha \\cdot V}$ (where $N_c$ is frequency of $c$, $V$ is vocabulary size) prevents this but introduces **bias** if we use the same alpha for different counts. All these small biases will later lead to significant error on many neutral tokens $c$ that are never met with `king` or `royalty`. To fix it, we can use our math skills and knowledge about the simulation to figure out consistent formulas for normalization.\n",
    "\n",
    "Thanks to smoothing on pairs $p(c,w) = \\frac{N_{cw} + \\alpha}{N + \\alpha V}$, the conditional distribution is $p(c|w) = \\frac{N_{cw} + \\alpha}{xN + x\\alpha V}$. Since we know the proportion $N_W/N_w = 1/2$ and $N_w/N = x = 1/6$, to be consistent on $c$ such that $N_{cw} = N_{cW} = 0$, the normalization for $p(c|W)$  should be:\n",
    "$$p(c|W) = \\frac{0.5\\alpha}{N_W + \\frac{1}{12} \\alpha V} = \\frac{0.5\\alpha}{0.5 xN + 0.5 x \\alpha V} = \\frac{\\alpha}{xN + x \\alpha V} = p(c|w)$$\n",
    "\n",
    "This ensures that smoothing remains proportionally consistent across target words and their paraphrase contexts among words that never occur with $w$ and $W$ ($N_{wc} = N_{Wc} = 0$), preventing spurious bias in the $\\sigma$ term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_all(counts, paired_counts, target_conditioned_counts, total, alpha=1, rank=50):\n",
    "    vocab = sorted(counts.keys())\n",
    "    word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "    n = len(vocab)\n",
    "    \n",
    "    p_w = np.array([(counts[w] + alpha) / (total + alpha * n) for w in vocab])\n",
    "    \n",
    "    p_cw = np.zeros((n, n))\n",
    "    for (w1, w2), cnt in paired_counts.items():\n",
    "        i, j = word2idx[w1], word2idx[w2]\n",
    "        p_cw[i, j] = cnt\n",
    "        p_cw[j, i] = cnt\n",
    "    p_cw = (p_cw + alpha) / (total + alpha * n)\n",
    "    \n",
    "    PMI = np.log(p_cw / (p_w[:, None] * p_w[None, :]))\n",
    "    \n",
    "    PAIRS = [('<royalty>', 'man'), ('<royalty>', 'woman')]\n",
    "    p_W = {pair: paired_counts[pair] / total for pair in PAIRS}\n",
    "    \n",
    "    p_c_given_W = {}\n",
    "    for pair in PAIRS:\n",
    "        tc = target_conditioned_counts[pair]\n",
    "        sz = paired_counts[pair]\n",
    "        p_c_given_W[pair] = {c: (tc.get(c, 0) + 0.5 * alpha) / (sz + alpha * n / 12) for c in vocab}\n",
    "    \n",
    "    U, S, Vt = np.linalg.svd(PMI, full_matrices=False)\n",
    "    sqrt_S = np.sqrt(np.abs(S[:rank])) * np.sign(S[:rank])\n",
    "    W = (U[:, :rank] * sqrt_S).T\n",
    "    C = (Vt[:rank, :].T * sqrt_S).T\n",
    "    C_dag = np.linalg.pinv(C.T)\n",
    "    \n",
    "    return PMI, p_cw, p_w, p_c_given_W, p_W, W, C_dag, word2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verification**\n",
    "\n",
    "Let's calculate all errors and check that actual difference of vectors and embeddings is explained by terms from the paper, and that linear analogies really emerge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify(PMI, p_cw, p_w, p_c_given_W, p_W, W, C_dag, word2idx):\n",
    "    n = len(word2idx)\n",
    "    idx = word2idx\n",
    "    vocab = {i: w for w, i in idx.items()}\n",
    "    log = lambda x: np.log(np.maximum(x, 1e-15))\n",
    "    \n",
    "    p_given_c = {w: p_cw[:, i] / p_w[i] for w, i in idx.items()}\n",
    "    \n",
    "    PAIRS = [('<royalty>', 'man'), ('<royalty>', 'woman')]\n",
    "    p_cW = {pair: np.array([p_c_given_W[pair][vocab[i]] for i in range(n)]) for pair in PAIRS}\n",
    "    p_Wc = {pair: p_cW[pair] * p_W[pair] / p_w for pair in PAIRS}\n",
    "    \n",
    "    CASES = [('king', 'man', '<royalty>', ('<royalty>', 'man')),\n",
    "             ('queen', 'woman', '<royalty>', ('<royalty>', 'woman'))]\n",
    "    \n",
    "    for space_name, vecs, project, s in [(\"PMI space\", PMI, lambda x: x, \"I\"),\n",
    "                                         (\"Embedding space\", W, lambda x: C_dag @ x, \"C†\")]:\n",
    "        print(f\"\\n{space_name}\")\n",
    "        print(\"=\" * 50)\n",
    "        for target, w1, w2, pair in CASES:\n",
    "            obs = vecs[:, idx[target]] - vecs[:, idx[w1]] - vecs[:, idx[w2]]\n",
    "            rho = log(p_given_c[target]) - log(p_cW[pair])\n",
    "            p_w1c = np.array([p_given_c[c][idx[w1]] for c in idx])\n",
    "            p_w2c = np.array([p_given_c[c][idx[w2]] for c in idx])\n",
    "            sigma = log(p_Wc[pair]) - log(p_w1c) - log(p_w2c)\n",
    "            tau = log(p_W[pair]) - log(p_w[idx[w1]]) - log(p_w[idx[w2]])\n",
    "            residual = np.linalg.norm(obs - project(rho + sigma - tau))\n",
    "            print(f\"{target} = {w1} + {w2}\")\n",
    "            print(f\"  ||ρ||={np.linalg.norm(rho):.2f}, ||σ||={np.linalg.norm(sigma):.2f}, |τ|={np.abs(tau):.4f}\")\n",
    "            print(f\"  residual: {residual:.6f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"ANALOGY: king - man + woman → ?\")\n",
    "    analogy = W[:, idx['king']] - W[:, idx['man']] + W[:, idx['woman']]\n",
    "    dists = [np.linalg.norm(analogy - W[:, i]) for i in range(n)]\n",
    "    for r, i in enumerate(np.argsort(dists)[:5]):\n",
    "        print(f\"  {r+1}. {vocab[i]:<12} dist={dists[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PMI space\n",
      "==================================================\n",
      "king = man + <royalty>\n",
      "  ||ρ||=0.56, ||σ||=15.08, |τ|=0.0079\n",
      "  residual: 0.000000\n",
      "queen = woman + <royalty>\n",
      "  ||ρ||=0.64, ||σ||=15.10, |τ|=0.0166\n",
      "  residual: 0.000000\n",
      "\n",
      "Embedding space\n",
      "==================================================\n",
      "king = man + <royalty>\n",
      "  ||ρ||=0.56, ||σ||=15.08, |τ|=0.0079\n",
      "  residual: 0.000000\n",
      "queen = woman + <royalty>\n",
      "  ||ρ||=0.64, ||σ||=15.10, |τ|=0.0166\n",
      "  residual: 0.000000\n",
      "\n",
      "==================================================\n",
      "ANALOGY: king - man + woman → ?\n",
      "  1. queen        dist=0.0585\n",
      "  2. <royalty>    dist=3.1119\n",
      "  3. royalty_9    dist=3.7952\n",
      "  4. royalty_15   dist=3.7968\n",
      "  5. royalty_1    dist=3.7990\n"
     ]
    }
   ],
   "source": [
    "N = 100_000\n",
    "counts, paired_counts, target_conditioned_counts = generate_data(n_windows=N, seed=42, scale=15)\n",
    "PMI, p_cw, p_w, p_c_given_W, p_W, W, C_dag, word2idx = build_all(\n",
    "    counts, paired_counts, target_conditioned_counts, N, rank=5\n",
    ")\n",
    "verify(PMI, p_cw, p_w, p_c_given_W, p_W, W, C_dag, word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results:**\n",
    "\n",
    "With proper construction we achieved:\n",
    "- $\\|\\rho\\| \\approx 0$ - paraphrase quality error is almost zero\n",
    "- $|\\tau| \\approx 0$ - words in paraphrase are marginally independent\n",
    "- Residual $= 0$ - all error terms from the paper correctly decompose the embedding distance\n",
    "- Analogy \"king - man + woman\" → queen ranks #1 — linear analogies emerge as predicted\n",
    "\n",
    "Note: $\\sigma$ isn't zero and it's very hard to make it so. Try inventing custom sampling that achieves $\\sigma \\approx 0$ to understand why it's so challenging :)\n",
    "\n",
    "**PS:** $x$ doesn't actually have to lead to $\\tau = 0$ — thanks to symmetry in our data, we would still have zero residual even if $\\tau \\neq 0$ (we would only face some difficulties with smoothing and making $\\rho \\approx 0$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
